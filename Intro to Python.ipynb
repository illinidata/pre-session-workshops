{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Python\n",
    "Author: Yizhi Fang\n",
    "1. [Anaconda](https://www.anaconda.com/distribution/)\n",
    "2. [Python documentation](https://www.python.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number\n",
    "a = 2\n",
    "b = 3\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# String\n",
    "a = 'hello'\n",
    "b = ' world'\n",
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List\n",
    "a = [1, 2]\n",
    "b = [3]\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original list: [1, 2, 3]\n",
      "first element: 1\n",
      "first two elements: [1, 2]\n",
      "append 4 at the end: [1, 2, 3, 4]\n",
      "add 5 at the end: [1, 2, 3, 4, 5]\n",
      "after removing value 2: [1, 3, 4, 5]\n",
      "pop the last element: 5\n",
      "after poping: [1, 3, 4]\n",
      "modify the first element: [10, 3, 4]\n",
      "11 is not in the list\n"
     ]
    }
   ],
   "source": [
    "# basic operations\n",
    "a = [1, 2, 3]\n",
    "print('original list:', a)\n",
    "\n",
    "print('first element:', a[0])\n",
    "\n",
    "print('first two elements:', a[0:2])\n",
    "\n",
    "a.append(4)\n",
    "print('append 4 at the end:', a)\n",
    "\n",
    "a += [5]\n",
    "print('add 5 at the end:', a)\n",
    "\n",
    "a.remove(2)\n",
    "print('after removing value 2:', a)\n",
    "\n",
    "print('pop the last element:', a.pop())\n",
    "print('after poping:', a)\n",
    "\n",
    "a[0] = 10\n",
    "print('modify the first element:', a)\n",
    "\n",
    "# check if an element is in a list\n",
    "if 11 in a:\n",
    "    print('11 is in the list')\n",
    "else:\n",
    "    print('11 is not in the list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# use append\n",
    "a = []\n",
    "for i in range(1, 11):\n",
    "    a.append(i)\n",
    "print(a)\n",
    "\n",
    "# use list summation\n",
    "a = []\n",
    "for i in range(1, 11):\n",
    "    a += [i]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n"
     ]
    }
   ],
   "source": [
    "# list comprehension\n",
    "a = list(range(10))\n",
    "print(a)\n",
    "\n",
    "b = [i**2 for i in a]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set doesn't support indexing\n",
    "for n in a:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original set: {1, 2, 3}\n",
      "add 4 in the set: {1, 2, 3, 4}\n",
      "remove 1 from the set: {2, 3, 4}\n",
      "11 is not in the set\n"
     ]
    }
   ],
   "source": [
    "# basic operations\n",
    "a = {1, 3, 2, 1}\n",
    "print('original set:', a)\n",
    "\n",
    "a.add(4)\n",
    "print('add 4 in the set:', a)\n",
    "\n",
    "a.remove(1)\n",
    "print('remove 1 from the set:', a)\n",
    "\n",
    "# check if an element is in a set\n",
    "if 11 in a:\n",
    "    print('11 is in the set')\n",
    "else:\n",
    "    print('11 is not in the set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand: Ford\n",
      "all keys: dict_keys(['brand', 'year', 'color'])\n",
      "all values: dict_values(['Ford', '2014', 'red'])\n",
      "(key, value) pairs: dict_items([('brand', 'Ford'), ('year', '2014'), ('color', 'red')])\n",
      "add size: {'brand': 'Ford', 'year': '2014', 'color': 'red', 'size': 'compact'}\n",
      "remove color: {'brand': 'Ford', 'year': '2014', 'size': 'compact'}\n",
      "brand Ford\n",
      "year 2014\n",
      "size compact\n"
     ]
    }
   ],
   "source": [
    "# basic operations\n",
    "a = {\n",
    "    'brand': 'Ford',\n",
    "    'year': '2014',\n",
    "    'color': 'red'\n",
    "}\n",
    "\n",
    "print('brand:', a['brand'])\n",
    "\n",
    "print('all keys:', a.keys())\n",
    "\n",
    "print('all values:', a.values())\n",
    "\n",
    "print('(key, value) pairs:', a.items())\n",
    "\n",
    "a['size'] = 'compact'\n",
    "print('add size:', a)\n",
    "\n",
    "a.pop('color')\n",
    "print('remove color:', a)\n",
    "\n",
    "for key in a:\n",
    "    print(key, a[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# find all even numbers\n",
    "a = list(range(1, 11))\n",
    "\n",
    "i = 0\n",
    "while i < len(a):\n",
    "    if not a[i]%2:\n",
    "        print(a[i])\n",
    "#     if a[i] > 8:\n",
    "#         break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "4\n",
      "6\n",
      "8\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i, n in enumerate(a):\n",
    "#     print(i, n)\n",
    "    if not n%2:\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8, 10]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_even(a):\n",
    "    res = []\n",
    "    for n in a:\n",
    "        if not n%2:\n",
    "            res.append(n)\n",
    "    return res\n",
    "\n",
    "find_even(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log scale: [0.         0.         0.69314718 1.09861229 1.38629436 1.60943791\n",
      " 1.79175947 1.94591015 2.07944154 2.19722458 2.30258509]\n",
      "x 2: [ 2  2  4  6  8 10 12 14 16 18 20]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "\n",
    "print('log scale:', np.log(a))\n",
    "\n",
    "b = np.array([2])\n",
    "print('x 2:', a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 5.090909090909091\n",
      "median: 5.0\n",
      "sum: 56\n"
     ]
    }
   ],
   "source": [
    "print('mean:', np.mean(a))\n",
    "\n",
    "print('median:', np.median(a))\n",
    "\n",
    "print('sum:', np.sum(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [10]]\n"
     ]
    }
   ],
   "source": [
    "# column vector\n",
    "a = a.reshape((len(a), 1))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first word: hello\n",
      "list of characters: ['h', 'e', 'l', 'l', 'o', '|', 'w', 'o', 'r', 'l', 'd']\n",
      "list of words: ['hello', 'world']\n",
      "replace l with space: he  o|wor d\n"
     ]
    }
   ],
   "source": [
    "s = 'hello|world'\n",
    "\n",
    "print('first word:', s[:5])\n",
    "\n",
    "print('list of characters:', list(s))\n",
    "\n",
    "print('list of words:', s.split('|'))\n",
    "\n",
    "print('replace l with space:', s.replace('l', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expression\n",
    "[Documentation](https://docs.python.org/3/howto/regex.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measurement: ['21.55', '23.45']\n",
      "The length is ..., ... cm.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "s = 'The length is 21.55, 23.45 cm.'\n",
    "print('measurement:', re.findall('\\d+\\.\\d+', s))\n",
    "\n",
    "s = re.sub('\\d+\\.\\d+', '...', s)\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_sre.SRE_Match object; span=(0, 24), match='ellen_walker10@gmail.com'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'ellen_walker10@gmail.com'\n",
    "re.search('[a-z0-9-_.]+@[a-z.]+', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "[Documentation](https://www.nltk.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "s = 'NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.'\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK is a leading platform for building Python programs to work with human language data.', 'It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.']\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(s)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(sents[0])\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pos tags\n",
    "1. [Full list of pos tags](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "2. Applications refer to [Chunking with regular expression](https://www.nltk.org/book/ch07.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('leading', 'VBG'),\n",
       " ('platform', 'NN'),\n",
       " ('for', 'IN'),\n",
       " ('building', 'VBG'),\n",
       " ('Python', 'NNP'),\n",
       " ('programs', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('work', 'VB'),\n",
       " ('with', 'IN'),\n",
       " ('human', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and lemmatization\n",
    "[Documentation](https://textminingonline.com/dive-into-nltk-part-iv-stemming-and-lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('his')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'going'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "wordnet_lemmatizer.lemmatize('going')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "[Wikipedia](https://en.wikipedia.org/wiki/Stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'leading',\n",
       " 'platform',\n",
       " 'building',\n",
       " 'Python',\n",
       " 'programs',\n",
       " 'work',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "1. [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "2. [Sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=0.2, max_df=1.0,\n",
    "                             ngram_range=(1, 3),\n",
    "                             max_features=1000,\n",
    "                             stop_words=stop_words,\n",
    "                             use_idf=True)\n",
    "X = vectorizer.fit_transform(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x110 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 110 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['50',\n",
       " '50 corpora',\n",
       " '50 corpora lexical',\n",
       " 'active',\n",
       " 'active discussion',\n",
       " 'active discussion forum',\n",
       " 'along',\n",
       " 'along suite',\n",
       " 'along suite text',\n",
       " 'building',\n",
       " 'building python',\n",
       " 'building python programs',\n",
       " 'classification',\n",
       " 'classification tokenization',\n",
       " 'classification tokenization stemming',\n",
       " 'corpora',\n",
       " 'corpora lexical',\n",
       " 'corpora lexical resources',\n",
       " 'data',\n",
       " 'discussion',\n",
       " 'discussion forum',\n",
       " 'easy',\n",
       " 'easy use',\n",
       " 'easy use interfaces',\n",
       " 'forum',\n",
       " 'human',\n",
       " 'human language',\n",
       " 'human language data',\n",
       " 'industrial',\n",
       " 'industrial strength',\n",
       " 'industrial strength nlp',\n",
       " 'interfaces',\n",
       " 'interfaces 50',\n",
       " 'interfaces 50 corpora',\n",
       " 'language',\n",
       " 'language data',\n",
       " 'leading',\n",
       " 'leading platform',\n",
       " 'leading platform building',\n",
       " 'lexical',\n",
       " 'lexical resources',\n",
       " 'lexical resources wordnet',\n",
       " 'libraries',\n",
       " 'libraries active',\n",
       " 'libraries active discussion',\n",
       " 'libraries classification',\n",
       " 'libraries classification tokenization',\n",
       " 'nlp',\n",
       " 'nlp libraries',\n",
       " 'nlp libraries active',\n",
       " 'nltk',\n",
       " 'nltk leading',\n",
       " 'nltk leading platform',\n",
       " 'parsing',\n",
       " 'parsing semantic',\n",
       " 'parsing semantic reasoning',\n",
       " 'platform',\n",
       " 'platform building',\n",
       " 'platform building python',\n",
       " 'processing',\n",
       " 'processing libraries',\n",
       " 'processing libraries classification',\n",
       " 'programs',\n",
       " 'programs work',\n",
       " 'programs work human',\n",
       " 'provides',\n",
       " 'provides easy',\n",
       " 'provides easy use',\n",
       " 'python',\n",
       " 'python programs',\n",
       " 'python programs work',\n",
       " 'reasoning',\n",
       " 'reasoning wrappers',\n",
       " 'reasoning wrappers industrial',\n",
       " 'resources',\n",
       " 'resources wordnet',\n",
       " 'resources wordnet along',\n",
       " 'semantic',\n",
       " 'semantic reasoning',\n",
       " 'semantic reasoning wrappers',\n",
       " 'stemming',\n",
       " 'stemming tagging',\n",
       " 'stemming tagging parsing',\n",
       " 'strength',\n",
       " 'strength nlp',\n",
       " 'strength nlp libraries',\n",
       " 'suite',\n",
       " 'suite text',\n",
       " 'suite text processing',\n",
       " 'tagging',\n",
       " 'tagging parsing',\n",
       " 'tagging parsing semantic',\n",
       " 'text',\n",
       " 'text processing',\n",
       " 'text processing libraries',\n",
       " 'tokenization',\n",
       " 'tokenization stemming',\n",
       " 'tokenization stemming tagging',\n",
       " 'use',\n",
       " 'use interfaces',\n",
       " 'use interfaces 50',\n",
       " 'wordnet',\n",
       " 'wordnet along',\n",
       " 'wordnet along suite',\n",
       " 'work',\n",
       " 'work human',\n",
       " 'work human language',\n",
       " 'wrappers',\n",
       " 'wrappers industrial',\n",
       " 'wrappers industrial strength']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec\n",
    "1. [Tutorial](https://adventuresinmachinelearning.com/word2vec-tutorial-tensorflow/)\n",
    "2. [Tutorial](https://www.tensorflow.org/tutorials/representation/word2vec)\n",
    "3. [Tensorflow](https://www.tensorflow.org/tutorials/representation/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
